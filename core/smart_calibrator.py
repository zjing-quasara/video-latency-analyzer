"""
æ™ºèƒ½æ ¡å‡†ç®¡ç†å™¨ - è‡ªåŠ¨å­¦ä¹ æœ€ä½³è¯†åˆ«å‚æ•°
"""
import cv2
import numpy as np
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class CalibrationResult:
    """æ ¡å‡†ç»“æœ"""
    app_strategy: str  # T_app æœ€ä½³ç­–ç•¥
    real_strategy: str  # T_real æœ€ä½³ç­–ç•¥
    app_confidence: float  # T_app å¹³å‡ç½®ä¿¡åº¦
    real_confidence: float  # T_real å¹³å‡ç½®ä¿¡åº¦
    success_rate: float  # æ€»ä½“æˆåŠŸç‡
    calibration_frames: int  # æ ¡å‡†ä½¿ç”¨çš„å¸§æ•°


class SmartCalibrator:
    """æ™ºèƒ½æ ¡å‡†å™¨ - è‡ªåŠ¨å¯»æ‰¾æœ€ä½³è¯†åˆ«å‚æ•°"""
    
    def __init__(self, ocr_engine, logger=None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ ¡å‡†å™¨
        
        Args:
            ocr_engine: è‡ªé€‚åº”OCRå¼•æ“å®ä¾‹
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.ocr_engine = ocr_engine
        self.logger = logger
        
        # æ ¡å‡†çŠ¶æ€
        self.is_calibrated = False
        self.calibration_result = None
        
        # è¿è¡Œæ—¶ç»Ÿè®¡
        self.app_success_count = 0
        self.real_success_count = 0
        self.total_frames = 0
        
    def _log(self, level: str, msg: str):
        """å†…éƒ¨æ—¥å¿—"""
        if self.logger:
            if level == 'debug':
                self.logger.debug(msg)
            elif level == 'info':
                self.logger.info(msg)
            elif level == 'warning':
                self.logger.warning(msg)
    
    def calibrate(self, video_path: str, app_roi: Tuple, max_frames: int = 10) -> CalibrationResult:
        """
        è‡ªåŠ¨æ ¡å‡† - ä½¿ç”¨å‰Nå¸§æ‰¾åˆ°æœ€ä½³è¯†åˆ«å‚æ•°
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            app_roi: T_app åŒºåŸŸ (x1, y1, x2, y2)
            max_frames: æœ€å¤šä½¿ç”¨å¤šå°‘å¸§è¿›è¡Œæ ¡å‡†ï¼ˆé»˜è®¤10å¸§ï¼‰
            
        Returns:
            æ ¡å‡†ç»“æœ
        """
        self._log('info', f"=" * 60)
        self._log('info', f"ğŸ”§ å¼€å§‹æ™ºèƒ½æ ¡å‡† - ä½¿ç”¨å‰ {max_frames} å¸§è‡ªåŠ¨å¯»æ‰¾æœ€ä½³å‚æ•°")
        self._log('info', f"=" * 60)
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        
        total_frames_in_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        self._log('info', f"è§†é¢‘ä¿¡æ¯: {total_frames_in_video} å¸§, {fps:.2f} fps")
        
        # æ™ºèƒ½é€‰æ‹©æ ¡å‡†å¸§ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        calibration_frames = self._select_calibration_frames(total_frames_in_video, max_frames)
        self._log('info', f"æ ¡å‡†å¸§ç´¢å¼•: {calibration_frames}")
        
        # æ”¶é›†è¯†åˆ«ç»“æœ
        app_results = defaultdict(lambda: {'success': 0, 'confidence': []})
        real_results = defaultdict(lambda: {'success': 0, 'confidence': []})
        
        for frame_idx in calibration_frames:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret:
                continue
            
            self._log('info', f"æ ¡å‡†å¸§ {frame_idx}...")
            
            # æµ‹è¯• T_app è¯†åˆ«ï¼ˆå›ºå®šåŒºåŸŸï¼‰
            if app_roi:
                x1, y1, x2, y2 = app_roi
                app_img = frame[y1:y2, x1:x2].copy()
                
                # å°è¯•æ‰€æœ‰ç­–ç•¥
                for strategy in ['original', 'contrast', 'sharpen', 'grayscale', 
                               'binary', 'binary_inv', 'denoise']:
                    result = self._test_strategy(app_img, strategy, 'T_app')
                    if result:
                        app_results[strategy]['success'] += 1
                        app_results[strategy]['confidence'].append(result['confidence'])
            
            # æµ‹è¯• T_real è¯†åˆ«ï¼ˆè‡ªåŠ¨æ£€æµ‹åŒºåŸŸï¼‰
            real_roi = self._quick_detect_real_roi(frame)
            if real_roi:
                x1, y1, x2, y2 = real_roi
                real_img = frame[y1:y2, x1:x2].copy()
                
                for strategy in ['original', 'contrast', 'sharpen', 'grayscale',
                               'binary', 'binary_inv', 'denoise']:
                    result = self._test_strategy(real_img, strategy, 'T_real')
                    if result:
                        real_results[strategy]['success'] += 1
                        real_results[strategy]['confidence'].append(result['confidence'])
        
        cap.release()
        
        # åˆ†æç»“æœï¼Œé€‰æ‹©æœ€ä½³ç­–ç•¥
        best_app_strategy = self._select_best_strategy(app_results, 'T_app')
        best_real_strategy = self._select_best_strategy(real_results, 'T_real')
        
        # è®¡ç®—æˆåŠŸç‡
        app_success_rate = app_results[best_app_strategy]['success'] / len(calibration_frames) if best_app_strategy else 0
        real_success_rate = real_results[best_real_strategy]['success'] / len(calibration_frames) if best_real_strategy else 0
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        app_conf = np.mean(app_results[best_app_strategy]['confidence']) if best_app_strategy and app_results[best_app_strategy]['confidence'] else 0
        real_conf = np.mean(real_results[best_real_strategy]['confidence']) if best_real_strategy and real_results[best_real_strategy]['confidence'] else 0
        
        overall_success = (app_success_rate + real_success_rate) / 2
        
        # åˆ›å»ºæ ¡å‡†ç»“æœ
        self.calibration_result = CalibrationResult(
            app_strategy=best_app_strategy or 'contrast',
            real_strategy=best_real_strategy or 'contrast',
            app_confidence=app_conf,
            real_confidence=real_conf,
            success_rate=overall_success,
            calibration_frames=len(calibration_frames)
        )
        
        self.is_calibrated = True
        
        # è¾“å‡ºæ ¡å‡†æŠ¥å‘Š
        self._log('info', f"=" * 60)
        self._log('info', f"âœ… æ ¡å‡†å®Œæˆï¼")
        self._log('info', f"=" * 60)
        self._log('info', f"ğŸ“Š T_app æœ€ä½³ç­–ç•¥: {self.calibration_result.app_strategy} "
                         f"(æˆåŠŸç‡: {app_success_rate:.1%}, ç½®ä¿¡åº¦: {app_conf:.2f})")
        self._log('info', f"ğŸ“Š T_real æœ€ä½³ç­–ç•¥: {self.calibration_result.real_strategy} "
                         f"(æˆåŠŸç‡: {real_success_rate:.1%}, ç½®ä¿¡åº¦: {real_conf:.2f})")
        self._log('info', f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {overall_success:.1%}")
        self._log('info', f"=" * 60)
        
        # å°†æœ€ä½³ç­–ç•¥è®¾ç½®åˆ°OCRå¼•æ“
        self.ocr_engine.best_strategy = self.calibration_result.app_strategy
        
        return self.calibration_result
    
    def _select_calibration_frames(self, total_frames: int, max_frames: int) -> List[int]:
        """æ™ºèƒ½é€‰æ‹©æ ¡å‡†å¸§ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰"""
        if total_frames <= max_frames:
            return list(range(min(10, total_frames)))
        
        # å‡åŒ€é€‰æ‹©
        step = total_frames // max_frames
        frames = [i * step for i in range(max_frames)]
        
        # ç¡®ä¿åŒ…å«ç¬¬ä¸€å¸§
        if 0 not in frames:
            frames[0] = 0
        
        return sorted(frames)
    
    def _test_strategy(self, img: np.ndarray, strategy: str, roi_type: str) -> Optional[Dict]:
        """æµ‹è¯•å•ä¸ªé¢„å¤„ç†ç­–ç•¥"""
        try:
            # é¢„å¤„ç†
            processed = self.ocr_engine.preprocess_image(img, strategy)
            
            # è½¬æ¢ä¸ºRGB
            if len(processed.shape) == 2:
                processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)
            elif processed.shape[2] == 3:
                processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)
            
            # OCRè¯†åˆ«ï¼ˆä¸ä¼ clså‚æ•°ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
            result = self.ocr_engine.ocr.ocr(processed)
            if not result or not result[0]:
                return None
            
            # æå–æ–‡æœ¬å’Œç½®ä¿¡åº¦
            texts = []
            confidences = []
            for item in result[0]:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    text_info = item[1]
                    if isinstance(text_info, (list, tuple)) and len(text_info) >= 1:
                        texts.append(str(text_info[0]))
                        if len(text_info) >= 2:
                            confidences.append(float(text_info[1]))
            
            # å°è¯•è§£ææ—¶é—´
            time_str = self.ocr_engine._parse_time_from_texts(texts)
            
            if time_str:
                avg_conf = np.mean(confidences) if confidences else 0
                return {
                    'time': time_str,
                    'confidence': avg_conf
                }
        
        except Exception as e:
            self._log('debug', f"ç­–ç•¥ {strategy} æµ‹è¯•å¼‚å¸¸: {e}")
        
        return None
    
    def _select_best_strategy(self, results: Dict, roi_type: str) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼ˆç»¼åˆæˆåŠŸç‡å’Œç½®ä¿¡åº¦ï¼‰"""
        if not results:
            return None
        
        # è®¡ç®—æ¯ä¸ªç­–ç•¥çš„ç»¼åˆå¾—åˆ†
        scores = {}
        for strategy, data in results.items():
            success_rate = data['success'] / 10  # æœ€å¤š10å¸§
            avg_conf = np.mean(data['confidence']) if data['confidence'] else 0
            
            # ç»¼åˆå¾—åˆ†ï¼šæˆåŠŸç‡ 70% + ç½®ä¿¡åº¦ 30%
            score = success_rate * 0.7 + avg_conf * 0.3
            scores[strategy] = score
            
            self._log('debug', f"[{roi_type}] ç­–ç•¥ '{strategy}': "
                             f"æˆåŠŸ={data['success']}, ç½®ä¿¡åº¦={avg_conf:.2f}, å¾—åˆ†={score:.2f}")
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        best = max(scores.items(), key=lambda x: x[1])
        return best[0] if best[1] > 0 else None
    
    def _quick_detect_real_roi(self, frame: np.ndarray) -> Optional[Tuple]:
        """å¿«é€Ÿæ£€æµ‹ T_real åŒºåŸŸï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        h, w = frame.shape[:2]
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å•ä¸€é˜ˆå€¼å¿«é€Ÿæ£€æµ‹
        _, th = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
        contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for cnt in contours:
            x, y, w_box, h_box = cv2.boundingRect(cnt)
            area = w_box * h_box
            
            # é¢ç§¯ç­›é€‰
            if 0.05 * w * h < area < 0.5 * w * h:
                # å®½é«˜æ¯”ç­›é€‰
                ratio = w_box / h_box if h_box > 0 else 0
                if 1.2 < ratio < 5:
                    return (x, y, x + w_box, y + h_box)
        
        return None
    
    def record_frame_result(self, app_success: bool, real_success: bool):
        """è®°å½•æ¯å¸§çš„è¯†åˆ«ç»“æœï¼ˆç”¨äºè¿è¡Œæ—¶ç›‘æ§ï¼‰"""
        self.total_frames += 1
        if app_success:
            self.app_success_count += 1
        if real_success:
            self.real_success_count += 1
    
    def get_runtime_stats(self) -> Dict:
        """è·å–è¿è¡Œæ—¶ç»Ÿè®¡"""
        if self.total_frames == 0:
            return {
                'app_success_rate': 0,
                'real_success_rate': 0,
                'overall_success_rate': 0
            }
        
        return {
            'app_success_rate': self.app_success_count / self.total_frames,
            'real_success_rate': self.real_success_count / self.total_frames,
            'overall_success_rate': (self.app_success_count + self.real_success_count) / (self.total_frames * 2)
        }
    
    def should_recalibrate(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ ¡å‡†ï¼ˆæˆåŠŸç‡ä¸‹é™ï¼‰"""
        if self.total_frames < 20:
            return False
        
        stats = self.get_runtime_stats()
        
        # å¦‚æœæˆåŠŸç‡ä½äº50%ï¼Œå»ºè®®é‡æ–°æ ¡å‡†
        if stats['overall_success_rate'] < 0.5:
            self._log('warning', f"âš ï¸ è¯†åˆ«æˆåŠŸç‡ä¸‹é™è‡³ {stats['overall_success_rate']:.1%}ï¼Œå»ºè®®æ£€æŸ¥è§†é¢‘è´¨é‡")
            return True
        
        return False



